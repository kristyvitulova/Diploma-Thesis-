import torch
import torch.nn as nn
import torch.optim as optim

# --- Model Definition with tuned dropout ---
class DeeperAutoencoderWithRegularization(nn.Module):
    def __init__(self):
        super().__init__()
        d = 0.07915602034842056  # dropout from trial 14
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1 = nn.Dropout(d)
        self.pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.drop2 = nn.Dropout(d)
        self.pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.drop3 = nn.Dropout(d)
        self.pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.unpool1 = nn.MaxUnpool2d(2, stride=2)
        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, padding=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4 = nn.Dropout(d)

        self.unpool2 = nn.MaxUnpool2d(2, stride=2)
        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, padding=1)
        self.bn5 = nn.BatchNorm2d(32)
        self.drop5 = nn.Dropout(d)

        self.unpool3 = nn.MaxUnpool2d(2, stride=2)
        self.deconv3 = nn.ConvTranspose2d(32, 1, 3, padding=1)

    def forward(self, x):
        orig = x.size()
        x = self.conv1(x); x = self.bn1(x); x = self.drop1(x); x, i1 = self.pool1(x); s1 = x.size()
        x = self.conv2(x); x = self.bn2(x); x = self.drop2(x); x, i2 = self.pool2(x); s2 = x.size()
        x = self.conv3(x); x = self.bn3(x); x = self.drop3(x); x, i3 = self.pool3(x); s3 = x.size()
        x = self.unpool1(x, i3, output_size=s2); x = self.deconv1(x); x = self.bn4(x); x = self.drop4(x)
        x = self.unpool2(x, i2, output_size=s1); x = self.deconv2(x); x = self.bn5(x); x = self.drop5(x)
        x = self.unpool3(x, i1, output_size=orig); x = self.deconv3(x)
        return x

# --- Training ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DeeperAutoencoderWithRegularization().to(device)

optimizer = optim.Adam(
    model.parameters(),
    lr=4.741390686132914e-05,
    weight_decay=2.9103837006459895e-06
)
criterion = nn.MSELoss()
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

num_epochs = 65
train_losses, val_losses = [], []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    for batch in train_loader:
        x = batch[0].to(device)
        optimizer.zero_grad()
        y = model(x)
        loss = criterion(y, x)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_losses.append(train_loss / len(train_loader))

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            x = batch[0].to(device)
            y = model(x)
            loss = criterion(y, x)
            val_loss += loss.item()
    val_losses.append(val_loss / len(val_loader))
    scheduler.step(val_losses[-1])

    print(f"[{epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}")

torch.save(model.state_dict(), "/home/kristyna/Desktop/autoencoder_ET_wave4_trial14.pth")
print("Model saved.")
