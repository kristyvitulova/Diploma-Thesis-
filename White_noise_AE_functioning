# Model Definition
# --------------------------
class DeeperAutoencoderWithRegularization(nn.Module):
    def __init__(self):
        super(DeeperAutoencoderWithRegularization, self).__init__()
        # Encoder
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.batchnorm1 = nn.BatchNorm2d(32)
        self.dropout1 = nn.Dropout(0.3)
        self.pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.batchnorm2 = nn.BatchNorm2d(64)
        self.dropout2 = nn.Dropout(0.3)
        self.pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.batchnorm3 = nn.BatchNorm2d(128)
        self.dropout3 = nn.Dropout(0.3)
        self.pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)

        # Decoder
        self.unpool1 = nn.MaxUnpool2d(2, stride=2)
        self.deconv1 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)
        self.batchnorm4 = nn.BatchNorm2d(64)
        self.dropout4 = nn.Dropout(0.3)

        self.unpool2 = nn.MaxUnpool2d(2, stride=2)
        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)
        self.batchnorm5 = nn.BatchNorm2d(32)
        self.dropout5 = nn.Dropout(0.3)

        self.unpool3 = nn.MaxUnpool2d(2, stride=2)
        self.deconv3 = nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1)

    def forward(self, x):
        orig_size = x.size()
        x = self.conv1(x); x = self.batchnorm1(x); x = self.dropout1(x); x, indices1 = self.pool1(x); size1 = x.size()
        x = self.conv2(x); x = self.batchnorm2(x); x = self.dropout2(x); x, indices2 = self.pool2(x); size2 = x.size()
        x = self.conv3(x); x = self.batchnorm3(x); x = self.dropout3(x); x, indices3 = self.pool3(x); size3 = x.size()
        x = self.unpool1(x, indices3, output_size=size2); x = self.deconv1(x); x = self.batchnorm4(x); x = self.dropout4(x)
        x = self.unpool2(x, indices2, output_size=size1); x = self.deconv2(x); x = self.batchnorm5(x); x = self.dropout5(x)
        x = self.unpool3(x, indices1, output_size=orig_size); x = self.deconv3(x)
        return x

# --------------------------
# STFT + Normalization
# --------------------------
def compute_spectrogram(waveform, nperseg=512, noverlap=256):
    f, t, Zxx = stft(waveform, nperseg=nperseg, noverlap=noverlap, fs=4096)
    return np.abs(Zxx)

def preprocess_waveforms_to_spectrograms(waveforms):
    processed = []
    for waveform in waveforms:
        spectrogram = compute_spectrogram(waveform.numpy())
        min_val = np.min(spectrogram)
        max_val = np.max(spectrogram)
        if max_val > min_val:
            spectrogram = (spectrogram - min_val) / (max_val - min_val)
        else:
            spectrogram = np.zeros_like(spectrogram)
        processed.append(spectrogram)
    return torch.tensor(processed, dtype=torch.float32)

def load_all_from_folder(folder_path):
    all_waveforms = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.pt'):
            path = os.path.join(folder_path, filename)
            data = torch.load(path)
            waveforms = data.get('waveforms', data) if isinstance(data, dict) else data
            all_waveforms.append(waveforms)
    return torch.cat(all_waveforms)

# --------------------------
# Load and Preprocess Noise Data
# --------------------------
noise_folder = "/home/kristyna/Desktop/only_noise2"
print(f"Loading noise-only data from: {noise_folder}")
raw_waveforms = load_all_from_folder(noise_folder)
spectrograms = preprocess_waveforms_to_spectrograms(raw_waveforms)
spectrograms = torch.nn.functional.interpolate(spectrograms.unsqueeze(1), size=(256, 31))

# Split into train / val
train_data, temp_data = train_test_split(spectrograms, test_size=0.3, random_state=42)
val_data, _ = train_test_split(temp_data, test_size=0.5, random_state=42)

train_loader = DataLoader(TensorDataset(train_data), batch_size=16, shuffle=True)
val_loader = DataLoader(TensorDataset(val_data), batch_size=16, shuffle=False)

# --------------------------
# Model Setup
# --------------------------
model = DeeperAutoencoderWithRegularization().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

# --------------------------
# Training Loop with Per-Sample RE Logging
# --------------------------
num_epochs = 30
train_losses, val_losses = [], []
train_errors_all, val_errors_all = [], []

for epoch in range(num_epochs):
    model.train()
    epoch_train_loss = 0.0
    epoch_train_errors = []

    for batch in train_loader:
        x = batch[0].to(device)
        optimizer.zero_grad()
        y = model(x)
        loss = criterion(y, x)
        loss.backward()
        optimizer.step()
        epoch_train_loss += loss.item()

        batch_errors = ((y - x) ** 2).mean(dim=[1, 2, 3])
        epoch_train_errors.extend(batch_errors.detach().cpu().numpy())

    train_losses.append(epoch_train_loss / len(train_loader))
    train_errors_all.extend(epoch_train_errors)

    # Validation
    model.eval()
    epoch_val_loss = 0.0
    epoch_val_errors = []

    with torch.no_grad():
        for batch in val_loader:
            x = batch[0].to(device)
            y = model(x)
            loss = criterion(y, x)
            epoch_val_loss += loss.item()
            batch_errors = ((y - x) ** 2).mean(dim=[1, 2, 3])
            epoch_val_errors.extend(batch_errors.detach().cpu().numpy())

    val_losses.append(epoch_val_loss / len(val_loader))
    val_errors_all.extend(epoch_val_errors)
    scheduler.step(epoch_val_loss)

    print(f"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}")

# --------------------------
# Save Trained Model
# --------------------------
model_path = "/home/kristyna/Desktop/autoencoder_4.pth"
torch.save(model.state_dict(), model_path)
print(f"Model saved to {model_path}")

# --------------------------
# Visualizations
# --------------------------

# Loss curves
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("train_val_loss_curve.pdf")
plt.show()

# --------------------------
# Final Evaluation on Noise-Only Data
# --------------------------
noise_loader = DataLoader(TensorDataset(spectrograms), batch_size=32, shuffle=False)
noise_only_errors = []

model.eval()
with torch.no_grad():
    for batch in noise_loader:
        x = batch[0].to(device)
        y = model(x)
        batch_errors = ((y - x) ** 2).mean(dim=[1, 2, 3])
        noise_only_errors.extend(batch_errors.cpu().numpy())

threshold = np.mean(noise_only_errors) + 3 * np.std(noise_only_errors)
print(f"Anomaly detection threshold: {threshold:.6f}")

# --- Extra Plot: Only Noise Reconstruction Errors ---
plt.figure(figsize=(8, 6))
plt.hist(noise_only_errors, bins=50, alpha=0.8, label='Noise-Only Errors', color='cornflowerblue')
plt.axvline(threshold, color='red', linestyle='--', label='Threshold')
plt.xlabel('Reconstruction Error')
plt.ylabel('Frequency')
plt.title('Distribution of Reconstruction Errors (Noise Only)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('error_distribution_noise_only.pdf')
plt.show()

# --------------------------
# Additional Evaluation: Noise+Signal Test Folder
# --------------------------
signal_folder = "/home/kristyna/Desktop/wave_with_noise2"
print(f"Loading noise+signal data from: {signal_folder}")
signal_waveforms = load_all_from_folder(signal_folder)
signal_specs = preprocess_waveforms_to_spectrograms(signal_waveforms)
signal_specs = torch.nn.functional.interpolate(signal_specs.unsqueeze(1), size=(256, 31))

signal_loader = DataLoader(TensorDataset(signal_specs), batch_size=32, shuffle=False)
signal_errors = []

model.eval()
with torch.no_grad():
    for batch in signal_loader:
        x = batch[0].to(device)
        y = model(x)
        batch_errors = ((y - x) ** 2).mean(dim=[1, 2, 3])
        signal_errors.extend(batch_errors.cpu().numpy())

# --------------------------
# Combined Histogram: Noise vs Signal+Noise
# --------------------------
plt.figure(figsize=(10, 6))
plt.hist(noise_only_errors, bins=50, alpha=0.7, label='Noise-Only Errors', color='cornflowerblue')
plt.hist(signal_errors, bins=50, alpha=0.6, label='Signal+Noise Errors', color='orange')
plt.axvline(threshold, color='red', linestyle='--', label='Anomaly Threshold')
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.title("Distribution of Reconstruction Errors: Noise vs Signal+Noise")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("comparison_histogram_noise_vs_signal.pdf")
plt.show()
